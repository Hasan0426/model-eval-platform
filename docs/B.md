æ˜¯çš„ï¼Œæ­å–œä½ ï¼**Milestone Bï¼šæ•°æ®é›†ä¸Šä¼ ä¸ Profiling** å·²ç»**å®Œå…¨å®Œæˆ**ã€‚

ç»è¿‡ä½ çš„è°ƒè¯•ï¼Œä½ ç°åœ¨æ‹¥æœ‰äº†ä¸€ä¸ªå¥å£®çš„ã€èƒ½å¤Ÿå¤„ç†è„æ•°æ®ï¼ˆNaNï¼‰ã€æ”¯æŒå¼‚æ­¥å¤„ç†ã€ä¸”å®Œå…¨æŒä¹…åŒ–çš„æ•°æ®åˆ†æå¹³å°ã€‚

ä»¥ä¸‹æ˜¯ Milestone B çš„è¯¦ç»†æŠ€æœ¯æ€»ç»“æŠ¥å‘Šã€‚æˆ‘å°†æŒ‰ç…§æ•°æ®æµå‘ï¼Œä¸ºä½ æ‹†è§£æ¯ä¸€ä¸ªæ ¸å¿ƒæ¨¡å—çš„ä»£ç é€»è¾‘å’Œå®ƒçš„ä½œç”¨ã€‚

---

# Milestone B å®Œæˆæƒ…å†µæ€»ç»“æŠ¥å‘Š

## 1. ç³»ç»Ÿæ¶æ„æ•°æ®æµ

1. **ç”¨æˆ·**é€šè¿‡ `POST /upload` ä¸Šä¼  CSVã€‚
2. **API** å°†åŸå§‹æ–‡ä»¶å­˜å…¥ **MinIO**ï¼ˆå¯¹è±¡å­˜å‚¨ï¼‰ã€‚
3. **API** åœ¨ **Postgres** åˆ›å»ºä¸€æ¡çŠ¶æ€ä¸º `PENDING` çš„è®°å½•ã€‚
4. **API** å‘é€ä»»åŠ¡ ID åˆ° **Redis**ï¼Œå¹¶ç«‹å³è¿”å›ã€‚
5. **Worker** æ”¶åˆ°ä»»åŠ¡ï¼Œä» **MinIO** ä¸‹è½½æ–‡ä»¶ã€‚
6. **Worker** ä½¿ç”¨ Pandas è®¡ç®— Schemaã€ç¼ºå¤±ç‡ã€åˆ†å¸ƒç­‰ã€‚
7. **Worker** æ¸…æ´—æ•°æ®ï¼ˆå¤„ç† NaNï¼‰ï¼Œå°† JSON ç»“æœæ›´æ–°å› **Postgres**ï¼ŒçŠ¶æ€å˜ä¸º `COMPLETED`ã€‚

---

## 2. è¯¦ç»†ä»£ç å®ç°ä¸è§£æ

### æ¨¡å—ä¸€ï¼šæ•°æ®åº“æ¨¡å‹ (`core/models.py`)

**ä½œç”¨**ï¼šå®šä¹‰äº†å…ƒæ•°æ®åœ¨æ•°æ®åº“é•¿ä»€ä¹ˆæ ·ã€‚è¿™æ˜¯ Postgres çš„â€œå›¾çº¸â€ã€‚

```python
from sqlalchemy import Column, String, Integer, DateTime, JSON, Text
from sqlalchemy.sql import func
from core.database import Base

class Dataset(Base):
    __tablename__ = "datasets"  # æ•°æ®åº“ä¸­çš„è¡¨å

    # --- åŸºç¡€ä¿¡æ¯ ---
    id = Column(String, primary_key=True, index=True) # UUIDï¼Œå”¯ä¸€æ ‡è¯†
    filename = Column(String, nullable=False)         # åŸå§‹æ–‡ä»¶å
    storage_path = Column(String, nullable=False)     # MinIO ä¸­çš„å­˜å‚¨è·¯å¾„ (key)
    
    # --- ä»»åŠ¡çŠ¶æ€æœº ---
    # PENDING (ç­‰å¾…ä¸­) -> PROCESSING (è®¡ç®—ä¸­) -> COMPLETED (å®Œæˆ) / FAILED (å¤±è´¥)
    status = Column(String, default="PENDING", index=True)
    
    # --- ç»Ÿè®¡å…ƒæ•°æ® ---
    row_count = Column(Integer, nullable=True) # è¡Œæ•°
    col_count = Column(Integer, nullable=True) # åˆ—æ•°
    file_size = Column(Integer, nullable=True) # æ–‡ä»¶å¤§å°(å­—èŠ‚)

    # --- æ ¸å¿ƒåˆ†æç»“æœ (å­˜ä¸º JSONB) ---
    # è¿™é‡Œå­˜å‚¨äº†æ‰€æœ‰è®¡ç®—å‡ºæ¥çš„å¤æ‚çš„åˆ†æç»“æœ
    schema_info = Column(JSON, nullable=True)       # æ¨æ–­å‡ºçš„å­—æ®µç±»å‹
    missing_stats = Column(JSON, nullable=True)     # ç¼ºå¤±å€¼ç»Ÿè®¡
    categorical_stats = Column(JSON, nullable=True) # ç±»åˆ«åˆ—åˆ†å¸ƒ (Top N)
    numerical_stats = Column(JSON, nullable=True)   # æ•°å€¼åˆ—åˆ†å¸ƒ (Mean/Std/Quantiles)
    preview = Column(JSON, nullable=True)           # å‰ N è¡Œæ•°æ®é¢„è§ˆ

    # --- é”™è¯¯è¿½è¸ª ---
    error_message = Column(Text, nullable=True)     # å¦‚æœä»»åŠ¡å¤±è´¥ï¼Œè®°å½•å †æ ˆä¿¡æ¯

    # --- æ—¶é—´æˆ³ ---
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())

```

---

### æ¨¡å—äºŒï¼šæ ¸å¿ƒåˆ†æå¼•æ“ (`core/profile.py`)

**ä½œç”¨**ï¼šè¿™æ˜¯â€œå¤§è„‘â€ã€‚è´Ÿè´£æŠŠ Pandas DataFrame å˜æˆ JSON ç»Ÿè®¡æ•°æ®ã€‚åŒ…å«æœ€é‡è¦çš„ NaN æ¸…æ´—é€»è¾‘ã€‚

```python
# ... å¼•ç”¨çœç•¥ ...

# ğŸ”¥ [å…³é”®ä¿®å¤] æ•°æ®æ¸…æ´—å™¨
def clean_nan(obj: Any) -> Any:
    """
    é€’å½’æ¸…æ´—æ•°æ®ï¼Œè§£å†³ Postgres JSONB ä¸æ”¯æŒ NaN/Infinity çš„é—®é¢˜ã€‚
    è¿™æ˜¯æœ¬æ¬¡ Debug çš„æ ¸å¿ƒäº§ç‰©ã€‚
    """
    if isinstance(obj, dict):
        return {k: clean_nan(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [clean_nan(v) for v in obj]
    elif isinstance(obj, (float, np.floating)):
        if math.isnan(obj) or math.isinf(obj):
            return None # å¼ºåˆ¶è½¬ä¸º JSON null
        return float(obj)
    # ... å…¶ä»–ç±»å‹å¤„ç† ...
    return obj

# 1. Schema æ¨æ–­
def infer_schema(df: pd.DataFrame, cfg: ProfileConfig) -> Dict[str, Any]:
    # è®¡ç®—åˆ—ç±»å‹ã€å”¯ä¸€å€¼æ•°é‡ã€æ˜¯å¦æ˜¯åˆ†ç±»å˜é‡
    # ... é€»è¾‘ ...
    return clean_nan({...}) # è¿”å›å‰å¿…é¡»æ¸…æ´—

# 2. æ•°å€¼ç»Ÿè®¡ (Milestone B æ–°å¢)
def numerical_stats(df: pd.DataFrame) -> Dict[str, Any]:
    # ç­›é€‰æ•°å€¼åˆ—
    num_df = df.select_dtypes(include=['number'])
    # ä½¿ç”¨ describe() å¿«é€Ÿè®¡ç®—å‡å€¼ã€æ ‡å‡†å·®ã€åˆ†ä½æ•°
    desc = num_df.describe().to_dict()
    # ... æ ¼å¼åŒ– ...
    return clean_nan(stats)

# 3. æ•°æ®é¢„è§ˆ
def get_preview(df: pd.DataFrame, rows: int = 10) -> List[Dict[str, Any]]:
    # æˆªå–å‰ 10 è¡Œï¼Œè½¬ä¸º List[Dict] ä¾›å‰ç«¯å±•ç¤º
    raw_preview = df.head(rows).to_dict(orient="records")
    return clean_nan(raw_preview)

```

---

### æ¨¡å—ä¸‰ï¼šå­˜å‚¨é€‚é…å±‚ (`core/storage.py`)

**ä½œç”¨**ï¼šå°è£…äº† MinIO çš„æ“ä½œã€‚ä¸šåŠ¡ä»£ç ä¸éœ€è¦çŸ¥é“åº•å±‚æ˜¯ S3 è¿˜æ˜¯ MinIOï¼Œåªç®¡è°ƒç”¨ä¸Šä¼ ä¸‹è½½ã€‚

```python
from minio import Minio
# ... é…ç½®å¼•ç”¨ ...

# åˆå§‹åŒ–å®¢æˆ·ç«¯
minio_client = Minio(...)

def upload_file(dataset_id: str, filename: str, data: bytes) -> str:
    """ä¸Šä¼ æ–‡ä»¶ï¼Œè¿”å›å­˜å‚¨è·¯å¾„"""
    ensure_bucket_exists() # ç¡®ä¿æ¡¶å­˜åœ¨
    object_name = f"{dataset_id}/{filename}" # ç”Ÿæˆè·¯å¾„ï¼šUUID/æ–‡ä»¶å.csv
    # ... put_object ...
    return object_name

def download_file(object_name: str) -> io.BytesIO:
    """ä¸‹è½½æ–‡ä»¶æµï¼Œä¾› Pandas è¯»å–"""
    # ... get_object ...
    return io.BytesIO(response.read())

```

---

### æ¨¡å—å››ï¼šAPI æ¥å£å±‚ (`api/main.py`)

**ä½œç”¨**ï¼šç³»ç»Ÿçš„é—¨é¢ã€‚è´Ÿè´£æ¥æ”¶è¯·æ±‚ã€æ ¡éªŒæ–‡ä»¶ã€å­˜æ•°æ®åº“ï¼ˆå ä½ï¼‰å¹¶æ´¾å‘ä»»åŠ¡ã€‚

```python
@app.post("/datasets/upload")
async def upload_dataset(file: UploadFile, db: Session = Depends(get_db)):
    # 1. æ ¡éªŒæ–‡ä»¶åç¼€
    if not file.filename.lower().endswith(".csv"):
        raise HTTPException(...)

    # 2. ä¸Šä¼ æ–‡ä»¶åˆ° MinIO (I/O æ“ä½œ)
    dataset_id = str(uuid.uuid4())
    content = await file.read()
    storage_path = storage.upload_file(dataset_id, file.filename, content)

    # 3. å†™å…¥æ•°æ®åº“ (çŠ¶æ€: PENDING)
    # æ­¤æ—¶è¿˜æ²¡æœ‰åˆ†æç»“æœï¼Œå…ˆå ä¸ªå‘
    new_dataset = Dataset(
        id=dataset_id,
        filename=file.filename,
        storage_path=storage_path,
        status="PENDING"
    )
    db.add(new_dataset)
    db.commit()

    # 4. è§¦å‘å¼‚æ­¥ä»»åŠ¡ (Fire and Forget)
    # å‘Šè¯‰ Workerï¼šæ‹¿ç€è¿™ä¸ª ID å»å¹²æ´»ï¼Œæˆ‘ä¸ç®¡ä½ äº†
    task_profile_dataset.delay(dataset_id)

    # 5. ç«‹å³å“åº”ç”¨æˆ·
    return {"dataset_id": dataset_id, "status": "PENDING"}

```

---

### æ¨¡å—äº”ï¼šWorker ä»»åŠ¡å±‚ (`worker/tasks.py`)

**ä½œç”¨**ï¼šåå°åŠ³æ¨¡ã€‚è´Ÿè´£æ‰§è¡ŒçœŸæ­£çš„è€—æ—¶è®¡ç®—ã€‚

```python
@celery_app.task(bind=True)
def task_profile_dataset(self, dataset_id: str):
    db = SessionLocal() # å»ºç«‹è‡ªå·±çš„æ•°æ®åº“è¿æ¥
    try:
        # 1. è·å–ä»»åŠ¡ä¿¡æ¯
        dataset = db.query(Dataset).filter(Dataset.id == dataset_id).first()
        dataset.status = "PROCESSING" # æ ‡è®°ä¸ºè¿›è¡Œä¸­
        db.commit()

        # 2. å‡†å¤‡æ•°æ®
        # ä» MinIO ä¸‹è½½ -> Pandas è¯»å–
        file_stream = storage.download_file(dataset.storage_path)
        df = pd.read_csv(file_stream)
        
        # 3. æ‰§è¡Œæ ¸å¿ƒåˆ†æé€»è¾‘ (CPU å¯†é›†å‹)
        # è¿™é‡Œè°ƒç”¨äº† core/profile.py é‡Œçš„æ‰€æœ‰å‡½æ•°
        num_stats = numerical_stats(df)
        preview = get_preview(df)
        # ... å…¶ä»–ç»Ÿè®¡ ...
        
        # 4. ä¿å­˜ç»“æœ
        dataset.numerical_stats = num_stats
        dataset.preview = preview
        # ... ä¿å­˜å…¶ä»–å­—æ®µ ...
        dataset.status = "COMPLETED" # æ ‡è®°å®Œæˆ
        db.commit()
        return "SUCCESS"

    except Exception as e:
        # 5. é”™è¯¯å¤„ç†ä¿åº•
        dataset.status = "FAILED"
        dataset.error_message = str(e) # è®°å½•æŠ¥é”™ä¿¡æ¯æ–¹ä¾¿æ’æŸ¥
        db.commit()
        raise e

```

---

## 3. éªŒæ”¶æ€»ç»“

| éªŒæ”¶é¡¹ | çŠ¶æ€ | è¯´æ˜ |
| --- | --- | --- |
| **CSV ä¸Šä¼ ** | âœ… | æ¥å£å“åº”æ­£å¸¸ï¼Œæ–‡ä»¶æˆåŠŸå­˜å…¥ MinIOã€‚ |
| **Schema è‡ªåŠ¨æ¨æ–­** | âœ… | èƒ½å¤Ÿè¯†åˆ«åˆ—ç±»å‹ã€Categorical åˆ—ã€‚ |
| **æ•°å€¼ç»Ÿè®¡** | âœ… | å‡å€¼ã€æ–¹å·®ã€åˆ†ä½æ•°è®¡ç®—æ­£ç¡®ã€‚ |
| **NaN å®¹é”™å¤„ç†** | âœ… | **æ ¸å¿ƒäº®ç‚¹**ã€‚è§£å†³äº† Numpy NaN ä¸ Postgres JSONB çš„å…¼å®¹æ€§é—®é¢˜ã€‚ |
| **å¼‚æ­¥æ¶æ„** | âœ… | API ç§’å›ï¼ŒWorker åå°å¤„ç†ï¼ŒRedis è°ƒåº¦æ­£å¸¸ã€‚ |
| **æ•°æ®æŒä¹…åŒ–** | âœ… | æœ€ç»ˆç»“æœæˆåŠŸå†™å…¥ Postgres æ•°æ®åº“ã€‚ |

**ä¸‹ä¸€æ­¥å»ºè®®ï¼š**
æ—¢ç„¶åœ°åŸºï¼ˆMilestone Aï¼‰å’Œæ•°æ®æ‘„å…¥åˆ†æï¼ˆMilestone Bï¼‰éƒ½å·²ç¨³å›ºï¼Œä½ å¯ä»¥è‡ªä¿¡åœ°å¼€å§‹ **Milestone C** çš„è§„åˆ’ï¼Œä¾‹å¦‚å®ç°**æ•°æ®åˆ‡ç‰‡ (Slicing)** æˆ–è€… **æ¨¡å‹è¯„ä¼°æŒ‡æ ‡è®¡ç®— (Metrics)**ã€‚