import os
import pandas as pd
import traceback
from worker.celery_app import celery_app
from core.database import SessionLocal
from core.models import Dataset
from core import storage
from core.profile import (
    ProfileConfig, infer_schema, validate_schema, 
    missing_stats, categorical_distributions, numerical_stats, get_preview
)
from core.models import TrainingJob # æ–°å¢å¼•ç”¨
# æ–°å¢å¼•ç”¨
from core.ml import train_transformer_model

from core.storage import download_file
@celery_app.task(bind=True)
def task_profile_dataset(self, dataset_id: str):
    # Worker éœ€è¦è‡ªå·±åˆ›å»º DB ä¼šè¯
    db = SessionLocal()
    try:
        # 1. è·å–ä»»åŠ¡è®°å½•
        dataset = db.query(Dataset).filter(Dataset.id == dataset_id).first()
        if not dataset:
            return "Dataset Not Found"
            
        # æ›´æ–°çŠ¶æ€ä¸º PROCESSING
        dataset.status = "PROCESSING"
        db.commit()

        # 2. ä» MinIO ä¸‹è½½æ–‡ä»¶
        file_stream = storage.download_file(dataset.storage_path)
        df = pd.read_csv(file_stream)
        
        # 3. æ‰§è¡Œ Profiling
        cfg = ProfileConfig()
        
        schema = infer_schema(df, cfg)
        validation = validate_schema(df)
        missing = missing_stats(df)
        cat_dist = categorical_distributions(df, cfg)
        num_stats = numerical_stats(df) # æ–°å¢
        preview = get_preview(df)       # æ–°å¢
        
        # 4. æ›´æ–°æ•°æ®åº“ç»“æœ
        dataset.row_count = int(df.shape[0])
        dataset.col_count = int(df.shape[1])
        dataset.schema_info = schema
        dataset.schema_check = validation # éœ€åœ¨ Model é‡ŒåŠ ä¸ªå­—æ®µï¼Œæˆ–è€…å­˜ schema_info é‡Œ
        dataset.missing_stats = missing
        dataset.categorical_stats = cat_dist
        dataset.numerical_stats = num_stats
        dataset.preview = preview
        
        dataset.status = "COMPLETED"
        db.commit()
        return "SUCCESS"

    except Exception as e:
        db.rollback()
        # è®°å½•é”™è¯¯ä¿¡æ¯
        if dataset:
            dataset.status = "FAILED"
            dataset.error_message = f"{str(e)}\n{traceback.format_exc()}"
            db.commit()
        raise e
    finally:
        db.close()
# worker/tasks.py

@celery_app.task(bind=True)
def task_train_model(self, job_id: str):
    db = SessionLocal() # ç§»é™¤ :Session ç±»å‹æ³¨è§£
    job = db.query(TrainingJob).filter(TrainingJob.id == job_id).first()
    
    # å˜é‡æ”¹ä¸ªåï¼Œæ˜ç¡®å®ƒæ˜¯æµï¼Œä¸æ˜¯è·¯å¾„
    file_stream = None 
    
    try:
        if not job:
            print(f"âŒ Job {job_id} not found.")
            return

        # æ›´æ–°çŠ¶æ€ä¸º RUNNING
        job.status = "RUNNING"
        db.commit()

        # 1. ä¸‹è½½æ•°æ®
        dataset = db.query(Dataset).filter(Dataset.id == job.dataset_id).first()
        if not dataset:
            raise ValueError(f"Dataset {job.dataset_id} not found")

        print(f"ğŸ“¥ Downloading dataset from: {dataset.storage_path}")
        # ğŸ”¥ è¿™é‡Œè¿”å›çš„æ˜¯ BytesIO å¯¹è±¡
        file_stream = download_file(dataset.storage_path)
        
        # ç›´æ¥è¯»å–æµ
        df = pd.read_csv(file_stream)

        # 2. è°ƒç”¨ Transformer è®­ç»ƒå‡½æ•°
        print(f"ğŸš€ Starting Transformer training for Job {job_id}...")
        run_id, metrics = train_transformer_model(
            df=df,
            target_col=job.target_col,
            task_type=job.task_type
        )

        # 3. æ›´æ–°æ•°æ®åº“
        job.mlflow_run_id = run_id
        job.metrics = metrics
        job.status = "COMPLETED"
        db.commit()
        print(f"âœ… Job {job_id} completed successfully.")

    except Exception as e:
        print(f"âŒ Training failed: {str(e)}")
        traceback.print_exc()
        try:
            job.status = "FAILED"
            job.error_message = str(e)
            db.commit()
        except:
            db.rollback()
    finally:
        db.close()
        # ğŸ”¥ğŸ”¥ğŸ”¥ åˆ é™¤äº† os.remove ä»£ç å— ğŸ”¥ğŸ”¥ğŸ”¥
        # å› ä¸ºå†…å­˜å¯¹è±¡ä¸éœ€è¦ï¼ˆä¹Ÿä¸èƒ½ï¼‰ç”¨ os.remove åˆ é™¤